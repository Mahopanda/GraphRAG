const { GoogleGenAI } = require("@google/genai");
require("dotenv").config();
const { LlmCache } = require("./llm-cache");

const apiKey = process.env.GEMINI_API_KEY;
let ai = apiKey ? new GoogleGenAI({ apiKey }) : null;

class LlmService {
  constructor(modelName = "gemini-2.5-flash-lite", enableCache = true) {
    this.modelName = modelName;
    this.cache = enableCache ? new LlmCache() : null;

    if (ai) {
      console.log(`LLM Service initialized with model: ${modelName}`);
      if (this.cache) {
        console.log("ğŸ’¾ LLM å¿«å–å·²å•Ÿç”¨");
        this.cache.cleanExpiredCache();
        const stats = this.cache.getCacheStats();
        console.log(
          `ğŸ“Š å¿«å–çµ±è¨ˆï¼š${stats.count} å€‹æª”æ¡ˆï¼Œ${stats.totalSizeMB} MB`
        );
      }
    } else {
      console.log(
        "LLM Service initialized without API key - features disabled"
      );
    }
  }

  // å°‡ä»»æ„è¼¸å…¥æ¨™æº–åŒ–æˆ contents[]
  _normalizeContents(input) {
    // 1) å­—ä¸² => å–®è¼ª user è¨Šæ¯
    if (typeof input === "string" && input.trim()) {
      return [{ role: "user", parts: [{ text: input }] }];
    }
    // 2) é™£åˆ— => é€ç­†è½‰ {role, parts:[{text}]}
    if (Array.isArray(input)) {
      return input.map((m) => {
        if (typeof m === "string") {
          return { role: "user", parts: [{ text: m }] };
        }
        // æ”¯æ´ { role, content } æˆ– { role, parts }
        if (m && m.role) {
          if (m.parts && Array.isArray(m.parts)) {
            // å°‡ assistant è§’è‰²è½‰æ›ç‚º model
            const role = m.role === "assistant" ? "model" : m.role;
            return { role, parts: m.parts };
          }
          const text = (m.content ?? "").toString();
          const role = m.role === "assistant" ? "model" : m.role;
          return { role, parts: [{ text }] };
        }
        // Fallback
        return { role: "user", parts: [{ text: JSON.stringify(m) }] };
      });
    }
    // 3) å…¶ä»–ç‰©ä»¶ => stringify ç•¶ä½œä¸€æ®µ user è¨Šæ¯
    return [{ role: "user", parts: [{ text: JSON.stringify(input) }] }];
  }

  async chat(promptOrHistory, timeoutMs = 30000) {
    if (!ai) {
      return "Error: LLM service is not available. Please set GEMINI_API_KEY.";
    }

    const contents = this._normalizeContents(promptOrHistory);

    // ç”Ÿæˆå¿«å–éµ
    const cacheKey = this.cache
      ? this.cache.generateCacheKey(contents, this.modelName)
      : null;

    // æª¢æŸ¥å¿«å–
    if (this.cache && cacheKey) {
      const cachedResponse = this.cache.getCache(cacheKey);
      if (cachedResponse) {
        return cachedResponse;
      }
    }

    // ç°¡å–®çš„ 503/UNAVAILABLE é‡è©¦ï¼ˆå«æŠ–å‹•ï¼‰
    const maxRetries = 4;
    for (let attempt = 0; attempt < maxRetries; attempt++) {
      try {
        // å»ºç«‹ timeout Promise
        const timeoutPromise = new Promise((_, reject) => {
          setTimeout(() => {
            reject(
              new Error(
                `Request timeout after ${timeoutMs}ms. è«‹ç¨å¾Œå†è©¦ï¼Œæˆ–æª¢æŸ¥ç¶²è·¯é€£ç·šã€‚`
              )
            );
          }, timeoutMs);
        });

        // å»ºç«‹ API å‘¼å« Promise
        const apiPromise = ai.models.generateContent({
          model: this.modelName,
          contents,
          // å¯é¸ï¼šåŠ ä¸Š generationConfig ä»¥é™ä½è² è¼‰å³°å€¼
          generationConfig: { temperature: 0.2, topK: 40, topP: 0.95 },
        });

        // ä½¿ç”¨ Promise.race ä¾†å¯¦ç¾ timeout
        const resp = await Promise.race([apiPromise, timeoutPromise]);
        const responseText = resp.text;

        // å„²å­˜åˆ°å¿«å–
        if (this.cache && cacheKey) {
          this.cache.setCache(cacheKey, responseText);
        }

        return responseText;
      } catch (err) {
        const msg = `${err?.status || ""} ${err?.message || err}`;

        // æª¢æŸ¥æ˜¯å¦ç‚º timeout éŒ¯èª¤
        if (msg.includes("timeout")) {
          console.error(`â° LLM å‘¼å«è¶…æ™‚ (${timeoutMs}ms):`, err.message);
          return `Error: LLM å‘¼å«è¶…æ™‚ã€‚è«‹ç¨å¾Œå†è©¦ï¼Œæˆ–æª¢æŸ¥ç¶²è·¯é€£ç·šã€‚è©³ç´°è³‡è¨Š: ${err.message}`;
        }

        const retriable =
          /UNAVAILABLE|DEADLINE_EXCEEDED|ECONNRESET|ETIMEDOUT/.test(msg);
        if (!retriable || attempt === maxRetries - 1) {
          console.error("Error calling Gemini API:", err);
          return `Error: LLM call failed. Details: ${msg}`;
        }
        const backoff =
          400 * Math.pow(2, attempt) + Math.floor(Math.random() * 200);
        console.log(
          `â³ é‡è©¦ ${attempt + 1}/${maxRetries}ï¼Œç­‰å¾… ${backoff}ms...`
        );
        await new Promise((r) => setTimeout(r, backoff));
      }
    }
  }
}

module.exports = { LlmService };
